---
permalink: /
hide_title: true
title: "home"
excerpt: "home"
author_profile: true
redirect_from: 
  - /home/
  - /home.html
---

I received my PhD from the [Computation and Neural Systems](https://www.bbe.caltech.edu/academics/cns) program at Caltech. I was advised by Professor [Pietro Perona](https://scholar.google.com/citations?user=j29kMCwAAAAJ&hl=en). I am currently on the job market, please reach out if you are looking for a researcher/engineer interested in interpretability and/or computer vision.

Research Interests
----
I’m broadly interested in how changes in model representations affect model behaviors. My work focuses on **representational alignment** and **interpretability**. How are these representations similar or different? How do these differences influence a model’s outputs? <br/><br/> 
To address these questions, I’ve been developing interpretability methods to analyze these similarities and differences across representations. Comparison is a powerful lens for understanding how changes in training, architecture, or data shape the representations models learn. Looking ahead, I plan to extend this work to compare human and model representations, uncovering concepts unique to each. This knowledge will help improve the alignment between how machines and humans represent the world, enabling the development of safer and more interpretable models.

Background
----
I earned a B.S. in Physiology & Neuroscience with a minor in Computer Science from the University of California San Diego. As a student at UCSD, I did systems neuroscience research in the Komiyama Lab for two years and neuroeconomics research with Dr. Pamela Reinagel for one year. During these years, I developed an interest in artificial neural networks and, upon graduation, decided to join the <a href="https://www.vision.caltech.edu">Computational Vision Group</a> at Caltech.
